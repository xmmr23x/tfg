\chapter{Estado de la técnica}
\label{ch:estado_tecnica}

En este capítulo se presentan los fundamentos teóricos más relevantes que sirven como base para el desarrollo de este proyecto. Se revisan conceptos clave relacionados con el aprendizaje automático y su aplicación en la detección de \textit{malware}, así como las nociones generales de ciberseguridad y los enfoques más utilizados en la identificación de software malicioso.

\section{Aprendizaje automático}
\label{sec:aprend_auto}

El aprendizaje automático se puede entender como <<la creación de algoritmos y modelos que permiten a los ordenadores aprender y hacer predicciones sin ser específicamente programados>> \cite{mwclass}.

\vspace{1em}

Inicialmente, los cálculos estadísticos se resolvían con máquinas electromecánicas, como la máquina tabuladora, desarrollada en 1890 por Herman Hollerith \cite{maquina_tabuladora}. Años más tarde, se presentó la neurona de McCulloch-Pitts, el primer modelo matemático de una neurona biológica, considerado por muchos como el punto de partida para el aprendizaje automático y la base de importantes modelos de cálculo \cite{inproceedings}. Hoy en día, el aprendizaje automático es esencial en diferentes ámbitos, como la investigación o los negocios, y emplea algoritmos avanzados capaces de hacer predicciones muy precisas sobre datos desconocidos.

\vspace{1em}

Los algoritmos desarrollados se pueden dividir en aprendizaje supervisado, no supervisado, semisupervisado y por refuerzo, entre otros. A su vez, y de manera independiente a la clasificación anterior, se pueden dividir en técnicas de clasificación y regresión, siendo las primeros las que nos ocupan en este proyecto. Algunas de las principales técnicas de clasificación ordinal son: árboles de decisión, redes neuronales artificiales, máquinas de vectores de soporte (SVM, por sus siglas en inglés) y algoritmos de agrupamiento.

\vspace{1em}

Todas estas técnicas se pueden adaptar a las necesidades actuales de la ciberseguridad. Los tipos de ataques, \textit{malware} o vulnerabilidades de los sistemas se están haciendo cada vez más frecuentes, no solo aumentando en cantidad, sino también en complejidad. Estos algoritmos pueden predecir si un \textit{software} es malicioso, si tiene vulnerabilidades o si un correo electrónico puede considerarse un intento de \textit{phishing}.

\vspace{1em}

A continuación se comentarán algunas de las principales técnicas y modelos que podrían llegar a aplicarse en este estudio si fuera necesario.

\subsection{Balanceo de datos}
\label{subsec:balanceo}

Es muy habitual que los conjuntos de datos se encuentren desbalanceados. Esto significa que la cantidad de patrones pertenecientes a una o varias clases son significativamente menores a la clase mayoritaria. Por ejemplo, como veremos más adelante en la tabla \ref{tabla:codificacion_malware}, la clase de patrones no malicionsos representa aproximadamente la mitad del total de patrones, en torno a 150000 patrones, mientras que la clase exploit solo tiene 12 patrones \cite{balanceo}. Debido a este desbalanceo es probable que la capacidad de generalización del modelo se vea afectada.

\vspace{1em}

Para mitigar estos problemas, las dos principales técnicas son el sobremuestreo y el submuestreo, \textit{oversampling} y \textit{undersampling} por sus términos en inglés.

\subsubsection{Sobremuestreo}
\label{subsubsec:oversampling}

Las técnicas de sobremuestreo, \textit{oversampling} en inglés, buscan aumentar la representación de la clase minoritaria generando nuevas instancias a partir de los casos existentes. El problema que presenta esta técnica es el riesgo de introducir información no real \cite{resamplig}. Entre los métodos de \textit{oversampling} más destacados se encuentran:

\begin{enumerate}
	\item \textbf{\textit{Random oversampling.}} \\
		Es la técnica más sencilla, ya que copia los patrones de la clase minoritaria hasta alcanzar la cantidad deseada. Es habitual que se haga hasta igualar a la clase mayoritaria \cite{resamplig}.
	\item \textbf{\textit{Synthetic Minority Oversampling Technique (SMOTE).}.} \\
		Genera instancias sintéticas de la clase minoritaria en lugar de replicar ejemplos existentes. Para ello, selecciona un ejemplo de la clase minoritaria y crea nuevos puntos interpolando con sus vecinos más cercanos \cite{ELREEDY201932}.
	\item \textbf{\textit{Adaptative Synthetic Sampling (ADASYN).}} \\
		Es una extensión de SMOTE que genera más instancias sintéticas en las zonas donde la clase minoritaria es más difícil de aprender, es decir, donde está menos representada respecto a la mayoritaria \cite{ADASYN}.
\end{enumerate}

\subsubsection{Submuestreo}
\label{subsubsec:undersampling}

El \textit{undersampling} engloba las técnicas que tratan de igualar las distribuciones de datos desbalanceados eliminando muestras de la clase mayoritaria respetando la distribución de la clase minoritaria. Las soluciones a este problema pueden cambiar en función del algoritmo que decide los patrones a eliminar \cite{resamplig}. Las principales técnicas de \textit{undersampling} son:

\begin{enumerate}
	\item \textbf{\textit{Random undersampling.}} \\
		Es el método más sencillo, ya que solo se encarga de eliminar patrones de forma aleatoria de la clase mayoritaria. Lo habitual, y la técnica empleada en este estudio para la clasificación binaria, es igualar el número de patrones para cada clase. Su principal punto en contra es que puede eliminar datos útiles \cite{rundersampling}.
	\item \textbf{\textit{Condensed nearest neighbours}.} \\
		Es un algoritmo no paramétrico basado en instancias, donde la clasificación se determina a partir de los k casos más cercanos al punto. Es una técnica local que utiliza medidas de distancia para identificar la similitud entre observaciones \cite{cnn}.
	\item \textbf{\textit{Tomek links.}} \\
		Consiste en identificar pares de instancias pertenecientes a clases distintas que son mutuamente los vecinos más cercanos entre sí. Suelen encontrarse en las zonas de solapamiento entre clases y representan casos ruidosos. Se elimina del conjunto de datos la instancia perteneciente a la clase mayoritaria en cada par, reduciendo así el desbalanceo \cite{tomeklinks}.
	\item \textbf{\textit{Edited nearest neighbours.}} \\
		Consiste en revisar cada instancia del conjunto de datos y clasificarla según la regla de los k vecinos más cercanos. Si la instancia no coincide con la clase mayoritaria de sus vecinos, se considera ruidosa o mal ubicada y se elimina del conjunto \cite{enn}.
\end{enumerate}

\newpage
\subsection{Reducción de la dimensionalidad}
\label{subsec:red_dim}

En estadística, la reducción de la dimensionalidad es el proceso por el cual se reduce el número de variables aleatorias. Si aplicamos esto al aprendizaje automático, el objetivo a reducir es el número de características de cada patrón. Generalmente se aplica antes de la clasificación para evitar los efectos de la maldición de la dimensionalidad. Esta maldición implica que cuando aumenta la dimensionalidad, el volumen del espacio aumenta exponencialmente haciendo que los datos se vuelvan dispersos \cite{maldicion}. La principal ventaja que aportan estas técnicas es reducir el tiempo de entrenamiento y la memoria utilizada en el mismo \cite{reddim}. A continuación, estudiaremos algunos de los principales métodos.

\begin{enumerate}
	\item \textbf{Análisis de componentes principales} \\
		El análisis de componentes principales se usa para describir un conjunto de datos en términos de nuevas características no correlacionadas, buscando la proyección donde los datos queden mejor representados según el método de mínimos cuadrados \cite{pca}.

	\item \textbf{Análisis factorial} \\
		Tiene el objetivo es identificar un conjunto reducido de factores latentes que explican la mayor parte de la varianza observada en los datos para descubrir las estructuras que generan las correlaciones entre las variables \cite{fa}.

	\item \textbf{Descomposición en valores singulares} \\
		Es una técnica algebraica que descompone una matriz en tres componentes: $U$, $\Sigma$ y $V^T$ \cite{dvs}.Esta descomposición permite representar los datos en un espacio reducido preservando la mayor parte de la información relevante.
\end{enumerate}

\subsection{Métricas de evaluación}
\label{subsec:2_metricas}

La elección de métricas de evaluación adecuadas es esencial para valorar de forma precisa el rendimiento de los modelos. No todas las métricas ofrecen la misma información. En esta sección se revisan las métricas más empleadas en la literatura especializada, destacando su utilidad, limitaciones y el tipo de información que aportan para la comparación de modelos.

\subsubsection{Exactitud}
\label{subsubsec:acc}

La exactitud o \textit{Accuracy} se corresponde con el porcentaje de aciertos que se han producido, es decir, los patrones clasificados correctamente respecto al total. Se calcula como la suma de verdaderos positivos (TP) y verdaderos negativos (TN) respecto al número total de patrones de entrada (N) \cite{metrics}.

\begin{equation}
	\label{eq:accuracy}
	\text{CCR} = \frac{TP+TN}{N}
\end{equation}

\subsubsection{Precisión}
\label{subsubsec:prec}

La precisión es una métrica que evalúa la proporción de patrones clasificados como positivas que realmente pertenecen a la clase positiva, es decir, mide como de confiable es el modelo cuando predice un positivo. Es muy relevante cuando el coste de clasificar erróneamente un negativo como positivo es alto.

\begin{equation}
	\text{Precisión} = \frac{TP}{TP + FP}
	\label{eq:precision}
\end{equation}

Donde \(TP\) representa el número de verdaderos positivos, y \(FP\) corresponde al número de falsos positivos.

\subsubsection{Sensibilidad}
\label{subsubsec:sens}

También conocida como exhaustividad o \textit{recall} en inglés, mide la capacidad del modelo para detectar correctamente los positivos de un conjunto de datos. Como se muestra en la ecuación \ref{eq:recall}, se calcula como la proporción entre el número de verdaderos positivos (TP) y la suma de verdaderos positivos y falsos negativos (FN) \cite{metrics}. Un valor alto de sensibilidad indica que se han obtenido pocos falsos negativos.

\begin{equation}
	\label{eq:recall}
	\text{Sensibilidad} = \frac{TP}{TP + FN}
\end{equation}

\subsubsection{Mínima sensibilidad}
\label{subsubsec:ms}

La mínima sensibilidad mide cómo de bien se clasifica la clase peor clasificada. Es útil en clasificación multiclase o con conjuntos de datos desbalanceados, ya que permite identificar si existe alguna clase que el modelo no está clasificando correctamente. Un valor alto indica que el modelo mantiene un buen rendimiento en todas las clases, mientras que un valor bajo revela que, al menos, una de ellas presenta un bajo grado de acierto. Si el modelo se deja una clase sin clasificar, el valor será 0.

\vspace{1em}

Sea \( S_i \) la sensibilidad de la clase \( i \), con \( n \) el número total de clases, la mínima sensibilidad se calcula como se muestra en la ecuación \ref{eq:ms}.

\begin{equation}
	MS = \min_{i \in \{1, 2, \dots, n\}} S_i
	\label{eq:ms}
\end{equation}

Donde la sensibilidad de cada clase \( S_i \) se obtiene mediante la ecuación \ref{eq:recall}

\newpage
\subsubsection{Valor-F}
\label{subsubsec:f1}

El valor-F o \textit{F1-score} mide el equilibrio entre la precisión y la sensibilidad \cite{metrics}. Se calcula como la media armónica entre ambas, lo que penaliza de forma más severa los valores extremos y proporciona una medida equilibrada del rendimiento del modelo. Es especialmente útil en problemas con clases desbalanceadas, ya que evita que un alto rendimiento en una sola métrica distorsione la evaluación global.

\subsubsection{Matriz de confusión}
\label{subsubsec:matrix}

Una matriz de confusión permite visualizar el rendimiento de un algoritmo de clasificación, normalmente supervisado. Cada fila representa las instancias en una clase real, mientras que cada columna representa las instancias en una clase predicha (o viceversa). La diagonal de la matriz representa las instancias correctamente clasificadas \cite{confmat}. Las matrices de confusión pueden utilizarse con cualquier algoritmo clasificador.

\subsection{Validación cruzada}
\label{subsec:validacion}

La validación es esencial en la experimentación con modelos de aprendizaje automático, ya que permite evaluar de manera fiable la capacidad de generalización del modelo, reduciendo el sesgo que podría aparecer al hacer una única partición entrenamiento/prueba. A continuación, se estudian las dos principales estrategias para este propósito.

\vspace{1em}

Se usa para para evaluar los resultados de un análisis estadístico y garantizar que son independientes de la partición entre datos de entrenamiento y prueba. Para ello hay que calcular la media aritmética obtenida de las medidas de evaluación sobre diferentes particiones. Este método es habitual cuando el objetivo es la predicción y se quiere estimar la precisión de los modelos que se van a utilizar \cite{crossval}. A continuación, se estudian las principales estrategias de validación cruzada.

\begin{enumerate}
	\item \textbf{Validación cruzada de K iteraciones} \\
		Los datos en k subconjuntos, usando en cada iteración uno como prueba y los restantes (k-1) como entrenamiento. El proceso se repite k veces y los resultados se promedian para obtener una única estimación del rendimiento. Ofrece mucha precisión, pero es muy costoso computacionalmente \cite{crossval1}.
	\item \textbf{Validación cruzada aleatoria} \\
		En este caso se eligen aleatoriamente los datos que pertenecen a cada partición de entrenamiento y prueba. El resultado es la media aritmética de los valores obtenidos para las métricas en cada iteración \cite{crossval2}.
	\item \textbf{Validación cruzada dejando uno fuera} \\
		En este tipo de validación se separan los datos de forma que tenemos una sola muestra para los datos de prueba, mientras que para los de entrenamiento tenemos el resto de patrones del conjunto de datos. Se realizan tantas iteraciones como muestras (N) tenga el conjunto de datos, calculando el error para cada una de ellas como evaluación y se obtiene la media aritmética de todas ellas \cite{crossval3}.

		\[
			E = \frac{1}{N} \sum_{i=1}^{N} E_{i}
		\]
\end{enumerate}

\subsection{Algoritmos de clasificación}
\label{subsec:clasificacion}
% TODO: 

\subsubsection{Árboles de decisión}
\label{subsubsec:arboles}

% TODO: 

\subsubsection{\textit{Random forest}}
\label{subsubsec:randomforest}

% TODO: 

\subsubsection{\textit{K-NN}}
\label{subsubsec:kneighbors}

% TODO: 

\subsubsection{\textit{Ridge}}
\label{subsubsec:ridge}

% TODO: 

\subsubsection{Perceptrón multicapa}
\label{subsubsec:mlp}

% TODO: 

\subsubsection{Máquinas de vectores de soporte}
\label{subsubsec:svm}

% TODO: 

\subsubsection{\textit{Light Gradient-Boosting Machine}}
\label{subsubsec:lgbm}

% TODO: 

\section{Ciberseguridad}
\label{sec:ciberseguridad}

La ciberseguridad es la protección de la infraestructura informática y la información que hay en ella, abarcando \textit{software}, \textit{hardware} y redes. Para garantizar la seguridad, es esencial combinar estrategias de prevención con métodos de protección efectivos. Las estrategias de prevención, como el uso de \textit{firewalls}, \textit{software} antivirus actualizado y educación en ciberseguridad para los usuarios, se centra en identificar y mitigar posibles amenazas antes de que ocurran. Por otro lado, la protección se enfoca en responder a los incidentes y minimizar sus efectos, mediante herramientas como los sistemas de detección de intrusiones. Con esto, podemos llegar a la conclusión de que el objetivo de la seguridad es minimizar los riesgos de recibir un ataque y reducir el impacto en caso recibirlo \cite{ciberseguridad_def}. En esta sección nos centraremos en la ciberseguridad \textit{software}, concretamente en los aspectos relacionados con la detección y clasificación de \textit{malware}.

\subsection{Conceptos generales}
\label{subsec:ciberseguridad_general}

La ciberseguridad constituye un pilar esencial en la sociedad digital actual, donde cada vez más actividades cotidianas, económicas y sociales dependen de sistemas informáticos y redes de comunicación. Su importancia radica en garantizar que la información y los servicios digitales sean fiables y estén protegidos frente a accesos indebidos o manipulaciones maliciosas.

\vspace{1em}

Los principales objetivos de la ciberseguridad se articulan en torno a la denominada tríada CIA \cite{triadacia}:

\begin{itemize}
	\item \textbf{Confidencialidad}: asegura que los datos solo sean accesibles por usuarios autorizados.
	\item \textbf{Integridad}: garantiza que la información no sea alterada de forma no autorizada.
	\item \textbf{Disponibilidad}: busca que los sistemas y servicios estén siempre accesibles.
\end{itemize}

En este contexto, las organizaciones y usuarios se enfrentan a amenazas comunes como que se comentarán superficialmente por su falta de relación con el proyecto. Algunas de las amenazas más frecuentes son:

\begin{enumerate}
	\item \textbf{\textit{Phishing}}: Correos electrónicos, mensajes o enlaces fraudulentos que buscan engañar al usuario para que revele información confidencial \cite{phishing}.
	\item \textbf{\textit{Malware}}: Software malicioso que diseñado para dañar sistemas, robar información o tomar control de dispositivos. Se explicará en profundidad en la sección \ref{subsec:malware}.
	\item \textbf{Denegación de servicio}: Inundación de sistemas con tráfico para interrumpir su funcionamiento y dejar servicios inaccesibles \cite{dos}.
	\item \textbf{Robo de credenciales/fuerza bruta}: Intentos de acceder a cuentas mediante contraseñas robadas, adivinación sistemática o explotación de vulnerabilidades de autenticación.
\end{enumerate}

\newpage
\subsection{\textit{Malware}}
\label{subsec:malware}

El \textit{software} malicioso o \textit{malware} es cualquier tipo de \textit{software} que se introduce sin el consentimiento del usuario con el objetivo de dañar o comprometer la confidencialidad, integridad o disponibilidad de la información o el sistema \cite{def_malware}. Su objetivo puede ir desde la interrupción del funcionamiento de sistemas hasta el robo de información sensible o la obtención de control remoto sobre dispositivos.

\vspace{1em}

Los primeros ejemplos de \textit{malware} surgieron en las décadas de 1970 y 1980 como experimentos de laboratorio o programas con fines demostrativos. Actualmente se ha convertido en una de las amenazas externas más relevantes debido al daño que puede llegar a causar y afectando tanto a usuarios individuales como a grandes corporaciones y entidades gubernamentales. Su relevancia se ha incrementado con la expansión de la digitalización y el uso masivo de Internet, donde ataques automatizados y campañas de \textit{malware} buscan constantemente vulnerabilidades en sistemas y redes, además de posibles descuidos de los usuarios.

\subsubsection{Tipos de \textit{Malware}}
\label{subsubsec:tipos_malware}

Podemos clasificar el \textit{malware} en diferentes categorías \cite{categoriamw} según su propósito:

\begin{itemize}
	\item Virus. Tienen como objetivo infectar archivos y sistemas informáticos. Se propagan cuando los usuarios comparten archivos o ejecutan programas infectados.
	\item Gusanos. Se propagan a través de las redes sin que tenga que intervenir el usuario.
	\item Troyanos. Se presentan como un \textit{software} legítimo. De esta forma intentan engañar al usuario para que lo descargue, instale y ejecute.
	\item \textit{Adware}. Muestra anuncios de forma intrusiva. Puede ser incrustada en una página web mediante gráficos, carteles, ventanas flotantes, o durante la instalación de algún programa al usuario, con el fin de generar lucro a sus autores.
	\item \textit{Spyware}. Trata de conseguir información de un equipo sin conocimiento ni consentimiento del usuario. Después transmite esta información a una entidad externa.
	\item \textit{Ransomware}. Conocido como secuestro de datos en español. Está diseñado para restringir el acceso a archivos o partes de un sistema y pedir un rescate para quitar la restricción.
	\item \textit{Rootkit}. Es un conjunto de \textit{software} que permite al atacante un acceso de privilegio a un ordenador, manteniendo presencia inicialmente oculta al control de los administradores.
	\item \textit{Keylogger}. Se encarga de registrar las pulsaciones que se realizan en el teclado, para memorizarlas en un fichero o enviarlas a través de Internet.
	\item \textit{Exploit}. Aprovecha un error o una vulnerabilidad de una aplicación o sistema para provocar un comportamiento involuntario.
	\item \textit{Backdoor}. Puerta trasera en español. Este tipo de \textit{software} permite un acceso no autorizado al sistema, evitando pasar por los métodos de autenticación.
\end{itemize}

\subsection{Técnicas de detección de \textit{Malware}}
\label{subsec:deteccion_malware}

Ningún método de detección es infalible y los principales antivirus comerciales pueden combinar distintas técnicas en función de las necesidades. La detección basada en firmas siguen siendo el método más usado en términos absolutos porque son rápidas, eficientes y fáciles de implementar. Este método consiste en comparar archivos con una base de datos de patrones conocidos. Otros mecanismos son: la detección heurística, por comportamiento, \textit{sandbox} e inteligencia artificial \cite{antivirus}.

\vspace{1em}

Existen varias limitaciones de los métodos tradicionales frente a nuevas amenazas. Por ejemplo, para evadir la detección basada en firmas se generaba una cadena de bits única cada vez que se codificaba. Esto se denomina polimorfismo. Gracias a la heurística no era necesaria una coincidencia exacta con las firmas almacenadas, pero debido a la gran cantidad de variaciones que surgen a diario, su efectividad y la de otros mecanismos se ve comprometida \cite{limitaciones}. A continuación se estudiarán algunas de las técnicas más usadas.

\subsubsection{Detección basada en firmas}
\label{subsubsec:firmas}

La detección de intrusiones basada en firmas compara la actividad del sistema con patrones conocidos de ataques para identificar comportamientos maliciosos. El código del \textit{Malware} de compila como cualquier programa, y este código se puede cifrar para crear una firma única. Otras formas de crear una firma son sus acciones en memoria o los archivos específicos que generan en ubicaciones específicas. En los casos más modernos se comunican a través de Internet hacia direcciones IP o dominios, lo que permite a los sistemas de defensa y prevención detectarlas y alertar \cite{firmas1}.

\vspace{1em}

Las firmas habitualmente son generadas y almacenadas por los proveedores comerciales de estos servicios, como pueden ser \textit{ESTET} \cite{eset} o \textit{Bitdefender} \cite{bitdefender}, aunque algunas pueden ser abiertas o colaborativas, como es el caso de \textit{VirusTotal} \cite{virustotal}.

\vspace{1em}

\paragraph*{\textbf{Ventajas.}}
Las detección basada en firmas es un sistema muy preciso, ya que si el autor del \textit{Malware} no realiza cambios en el código, la firma no cambia y la detección es casi inmediata. Esto implica que el sistema de detección proporcione muy pocos falsos positivos \cite{firmas1}.

\vspace{1em}

\paragraph*{\textbf{Inconvenientes.}}
El principal inconveniente de la detección basada en firmas es que para poder actuar, la amenaza debe ser conocida. Esto significa que no tienen forma de detectar las amenazas de tipo \textit{zero day}, es decir, que acaban de ser descubiertas y que aún no tienen una firma \cite{zeroday}.

\noindent Como se ha comentado en las ventajas, es casi infalible si no se modifica el código del programa, pero la firma queda completamente inservible con un pequeño cambio que se introduzca \cite{firmas}.

\subsubsection{Detección heurística}
\label{subsubsec:heuristica}

En la detección de \textit{Malware}, se conoce como heurística al conjunto de técnicas que se usan para identificar aplicaciones maliciosas que no se encuentran en la base de datos de firmas \cite{heu}. Surge de la necesidad de combatir las nuevas amenazas y es uno de los pocos métodos capaces de combatir los \textit{Malware} polimórficos, que cambian su código constantemente \cite{kp_heu}

\vspace{1em}

\paragraph*{\textbf{Ventajas.}}
La principal ventaja de la detección heurística es su capacidad para identificar nuevas amenazas desconocidas. Al no depender exclusivamente de firmas previas, resulta eficaz para reconocer modificaciones de \textit{Malware} existente o ataques que todavía no han sido documentados. Esto amplía la cobertura de protección frente a amenazas emergentes \cite{fortinet_heu}.

\paragraph*{\textbf{Inconvenientes.}}
Este tipo de análisis trata de detectar el comportamiento conocido, por lo que si un programa no realiza ninguna acción conocida, es probable que no sea detectado \cite{fortinet_heu}.

\subsubsection{Detección basada en comportamiento}
\label{subsubsec:comportamiento}

Esta técnica es clave frente a amenazas que pueden evitar ser detectadas por los métodos tradicionales. Los atacantes, para evitar ser detectados, ofuscan el código, práctica que ``consiste en transformar el código fuente de un programa en una forma más compleja y difícil de comprender, sin alterar su funcionalidad'' \cite{ofuscacion}.

\vspace{1em}

La detección basada en comportamiento analiza las actividades y patrones de ejecución de programas para identificar posibles amenazas y descubrir comportamientos anómalos que puedan indicar una intrusión, incluso cuando se trata de ataques desconocidos o de tipo \textit{zero day}. Es capaz de identificar amenazas desconocidas, ya que no depende de bases de datos de firmas previas pero suele generar un mayor número de falsos positivos.

\subsubsection{Detección mediante aprendizaje automático}
\label{subsubsec:ml}

Los modelos de aprendizaje automático tienen la capacidad de extraer conocimiento a partir de grandes cantidades de información y reconocer tanto patrones complejos como inusuales. Examina entradas como registros del sistema, tráfico de red, procesos en ejecución o secuencias de instrucciones para localizar conductas anómalas. Además, pueden ajustarse de forma continua incorporando nuevas muestras, lo que incrementa su efectividad frente a variantes no identificadas \cite{s2grupo}.

\vspace{1em}

En este caso, si un atacante realiza cambios en el código, lo ofusca o es un programa no conocido, no tiene por qué engañar al modelo. Esto hace que sea uno de los métodos de detección de \textit{malware} más potentes y con proyección de futuro \cite{ml_mw}.
